{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbhIJVfHgECE"
      },
      "source": [
        "# Web Scraping with Beautiful Soup\n",
        "\n",
        "* * *\n",
        "\n",
        "### Icons used in this notebook\n",
        "üîî **Question**: A quick question to help you understand what's going on.<br>\n",
        "ü•ä **Challenge**: Interactive exercise. We'll work through these in the workshop!<br>\n",
        "‚ö†Ô∏è **Warning**: Heads-up about tricky stuff or common mistakes.<br>\n",
        "üí° **Tip**: How to do something a bit more efficiently or effectively.<br>\n",
        "üé¨ **Demo**: Showing off something more advanced ‚Äì so you know what Python can be used for!<br>\n",
        "\n",
        "### Learning Objectives\n",
        "1. [Reflection: To Scape Or Not To Scrape](#when)\n",
        "2. [Extracting and Parsing HTML](#extract)\n",
        "3. [Scraping the Illinois General Assembly](#scrape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-AoHuK6gECF"
      },
      "source": [
        "<a id='when'></a>\n",
        "\n",
        "# To Scrape Or Not To Scrape\n",
        "\n",
        "When we'd like to access data from the web, we first have to make sure if the website we are interested in offers a Web API. Platforms like Twitter, Reddit, and the New York Times offer APIs. **Check out D-Lab's [Python Web APIs](https://github.com/dlab-berkeley/Python-Web-APIs) workshop if you want to learn how to use APIs.**\n",
        "\n",
        "However, there are often cases when a Web API does not exist. In these cases, we may have to resort to web scraping, where we extract the underlying HTML from a web page, and directly obtain the information we want. There are several packages in Python we can use to accomplish these tasks. We'll focus two packages: Requests and Beautiful Soup.\n",
        "\n",
        "Our case study will be scraping information on the [state senators of Illinois](http://www.ilga.gov/senate), as well as the [list of bills](http://www.ilga.gov/senate/SenatorBills.asp?MemberID=1911&GA=98&Primary=True) each senator has sponsored. Before we get started, peruse these websites to take a look at their structure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eE-_mm9fgECG"
      },
      "source": [
        "## Installation\n",
        "\n",
        "We will use two main packages: [Requests](http://docs.python-requests.org/en/latest/user/quickstart/) and [Beautiful Soup](http://www.crummy.com/software/BeautifulSoup/bs4/doc/). Go ahead and install these packages, if you haven't already:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_32kq1HXgECG",
        "outputId": "590abb48-84f1-4b45-bf93-389ca84a819d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.7.14)\n"
          ]
        }
      ],
      "source": [
        "# Instala el paquete 'requests', que permite hacer solicitudes HTTP en Python.\n",
        "# El prefijo %pip se usa en notebooks (como Google Colab) para ejecutar comandos de instalaci√≥n de paquetes directamente desde la celda.\n",
        "%pip install requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvWraAX6gECH",
        "outputId": "3c110dd2-d03f-4214-e9f1-92abb67693a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.14.1)\n"
          ]
        }
      ],
      "source": [
        "# Instala la biblioteca 'beautifulsoup4', que se utiliza para analizar (parsear) documentos HTML y XML.\n",
        "# Es muy √∫til para tareas de scraping o extracci√≥n de datos de p√°ginas web.\n",
        "%pip install beautifulsoup4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdQIXpAIgECH"
      },
      "source": [
        "We'll also install the `lxml` package, which helps support some of the parsing that Beautiful Soup performs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuiXDN0DgECH",
        "outputId": "32386b0e-aeb4-4964-977f-4c04002f1e47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (5.4.0)\n"
          ]
        }
      ],
      "source": [
        "# Instala el paquete 'lxml', un parser r√°pido y eficiente para analizar documentos HTML y XML.\n",
        "# BeautifulSoup puede usar 'lxml' como motor de an√°lisis para obtener mejores resultados en velocidad y precisi√≥n.\n",
        "%pip install lxml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "YwOhv2-9gECH"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "# Se importan las librer√≠as necesarias para realizar scraping web:\n",
        "# BeautifulSoup para analizar el contenido HTML, requests para hacer solicitudes HTTP,\n",
        "# datetime para manejar fechas y horas, y time para controlar pausas en la ejecuci√≥n.\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "import requests\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlLz5zSagECH"
      },
      "source": [
        "<a id='extract'></a>\n",
        "\n",
        "# Extracting and Parsing HTML\n",
        "\n",
        "In order to succesfully scrape and analyse HTML, we'll be going through the following 4 steps:\n",
        "1. Make a GET request\n",
        "2. Parse the page with Beautiful Soup\n",
        "3. Search for HTML elements\n",
        "4. Get attributes and text of these elements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSPbkglHgECH"
      },
      "source": [
        "## Step 1: Make a GET Request to Obtain a Page's HTML\n",
        "\n",
        "We can use the Requests library to:\n",
        "\n",
        "1. Make a GET request to the page, and\n",
        "2. Read in the webpage's HTML code.\n",
        "\n",
        "The process of making a request and obtaining a result resembles that of the Web API workflow. Now, however, we're making a request directly to the website, and we're going to have to parse the HTML ourselves. This is in contrast to being provided data organized into a more straightforward `JSON` or `XML` output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "zyb1FbbVgECI",
        "outputId": "de9202fb-6317-47c0-9fc5-890950bb85c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<!DOCTYPE html>\r\n",
            "<html lang=\"en\">\r\n",
            "<head id=\"Head1\">\r\n",
            "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\r\n",
            "    <meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8\" />\r\n",
            "    <meta http-equiv=\"X-UA-Compatible\" content=\"IE=Edge\" />\r\n",
            "    <meta charset=\"utf-8\" />\r\n",
            "    <meta charset=\"UTF-8\">\r\n",
            "    <!-- Meta Description -->\r\n",
            "    <meta name=\"description\" content=\"Welcome to the official government website of the Illinois General Assembly\">\r\n",
            "    <meta name=\"contactName\" content=\"Legislative Information System\">\r\n",
            "    <meta name=\"contactOrganization\" content=\"LIS Staff Services\">\r\n",
            "    <meta name=\"contactStreetAddress1\" content=\"705 Stratton Office Building\">\r\n",
            "    <meta name=\"contactCity\" content=\"Springfield\">\r\n",
            "    <meta name=\"contactZipcode\" content=\"62706\">\r\n",
            "    <meta name=\"contactNetworkAddress\" content=\"webmaster@ilga.gov\">\r\n",
            "    <meta name=\"contactPhoneNumber\" content=\"217-782-3944\">\r\n",
            "    <meta name=\"contactFaxNumber\" content=\"217-524-6059\">\r\n",
            "    <meta name\n"
          ]
        }
      ],
      "source": [
        "# Realiza una solicitud GET a la URL especificada para obtener el contenido de la p√°gina web,\n",
        "# guarda el texto HTML de la respuesta en la variable 'src' y muestra los primeros 1000 caracteres para una vista previa.\n",
        "# Esto permite verificar que la solicitud fue exitosa y se obtuvo contenido.\n",
        "\n",
        "# Make a GET request\n",
        "req = requests.get('http://www.ilga.gov/senate/default.asp')\n",
        "# Read the content of the server‚Äôs response\n",
        "src = req.text\n",
        "# View some output\n",
        "print(src[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEX47M36gECI"
      },
      "source": [
        "## Step 2: Parse the Page with Beautiful Soup\n",
        "\n",
        "Now, we use the `BeautifulSoup` function to parse the reponse into an HTML tree. This returns an object (called a **soup object**) which contains all of the HTML in the original document.\n",
        "\n",
        "If you run into an error about a parser library, make sure you've installed the `lxml` package to provide Beautiful Soup with the necessary parsing tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zIrHlZ9FgECI",
        "outputId": "c27bb535-ba2d-4705-92a8-8e4681071a41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<!DOCTYPE html>\n",
            "<html lang=\"en\">\n",
            " <head id=\"Head1\">\n",
            "  <meta content=\"width=device-width, initial-scale=1.0\" name=\"viewport\"/>\n",
            "  <meta content=\"text/html;charset=utf-8\" http-equiv=\"content-type\"/>\n",
            "  <meta content=\"IE=Edge\" http-equiv=\"X-UA-Compatible\"/>\n",
            "  <meta charset=\"utf-8\"/>\n",
            "  <meta charset=\"utf-8\"/>\n",
            "  <!-- Meta Description -->\n",
            "  <meta content=\"Welcome to the official government website of the Illinois General Assembly\" name=\"description\"/>\n",
            "  <meta content=\"Legislative Information System\" name=\"contactName\"/>\n",
            "  <meta content=\"LIS Staff Services\" name=\"contactOrganization\"/>\n",
            "  <meta content=\"705 Stratton Office Building\" name=\"contactStreetAddress1\"/>\n",
            "  <meta content=\"Springfield\" name=\"contactCity\"/>\n",
            "  <meta content=\"62706\" name=\"contactZipcode\"/>\n",
            "  <meta content=\"webmaster@ilga.gov\" name=\"contactNetworkAddress\"/>\n",
            "  <meta content=\"217-782-3944\" name=\"contactPhoneNumber\"/>\n",
            "  <meta content=\"217-524-6059\" name=\"contactFaxNumber\"/>\n",
            "  <meta content=\"State Of Illinois\" name=\"originatorJur\n"
          ]
        }
      ],
      "source": [
        "# Convierte el contenido HTML en un objeto BeautifulSoup usando el parser 'lxml',\n",
        "# lo que facilita la navegaci√≥n y extracci√≥n de datos del √°rbol HTML.\n",
        "# Luego imprime una versi√≥n formateada y legible (prettify) de los primeros 1000 caracteres del HTML.\n",
        "\n",
        "# Parse the response into an HTML tree\n",
        "soup = BeautifulSoup(src, 'lxml')\n",
        "# Take a look\n",
        "print(soup.prettify()[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmuRSA0wgECI"
      },
      "source": [
        "The output looks pretty similar to the above, but now it's organized in a `soup` object which allows us to more easily traverse the page."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5dDm4yEgECI"
      },
      "source": [
        "## Step 3: Search for HTML Elements\n",
        "\n",
        "Beautiful Soup has a number of functions to find useful components on a page. Beautiful Soup lets you find elements by their:\n",
        "\n",
        "1. HTML tags\n",
        "2. HTML Attributes\n",
        "3. CSS Selectors\n",
        "\n",
        "Let's search first for **HTML tags**.\n",
        "\n",
        "The function `find_all` searches the `soup` tree to find all the elements with an a particular HTML tag, and returns all of those elements.\n",
        "\n",
        "What does the example below do?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmTtPeIMgECI",
        "outputId": "708c363a-a9f8-4f55-b141-2fc31c0067bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<a b-0yw6sxot5c=\"\" class=\"dropdown-item\" data-lang=\"en\" href=\"#\">\n",
            "<span b-0yw6sxot5c=\"\" class=\"flag-icon flag-icon-us\"></span> English\r\n",
            "                            </a>, <a b-0yw6sxot5c=\"\" class=\"dropdown-item\" data-lang=\"af\" href=\"#\">\n",
            "<span b-0yw6sxot5c=\"\" class=\"flag-icon flag-icon-za\"></span> Afrikaans\r\n",
            "                            </a>, <a b-0yw6sxot5c=\"\" class=\"dropdown-item\" data-lang=\"sq\" href=\"#\">\n",
            "<span b-0yw6sxot5c=\"\" class=\"flag-icon flag-icon-al\"></span> Albanian\r\n",
            "                            </a>, <a b-0yw6sxot5c=\"\" class=\"dropdown-item\" data-lang=\"ar\" href=\"#\">\n",
            "<span b-0yw6sxot5c=\"\" class=\"flag-icon flag-icon-ae\"></span> Arabic\r\n",
            "                            </a>, <a b-0yw6sxot5c=\"\" class=\"dropdown-item\" data-lang=\"hy\" href=\"#\">\n",
            "<span b-0yw6sxot5c=\"\" class=\"flag-icon flag-icon-am\"></span> Armenian\r\n",
            "                            </a>, <a b-0yw6sxot5c=\"\" class=\"dropdown-item\" data-lang=\"az\" href=\"#\">\n",
            "<span b-0yw6sxot5c=\"\" class=\"flag-icon flag-icon-az\"></span> Azerbaijani\r\n",
            "                            </a>, <a b-0yw6sxot5c=\"\" class=\"dropdown-item\" data-lang=\"eu\" href=\"#\">\n",
            "<span b-0yw6sxot5c=\"\" class=\"flag-icon flag-icon-eu\"></span> Basque\r\n",
            "                            </a>, <a b-0yw6sxot5c=\"\" class=\"dropdown-item\" data-lang=\"bn\" href=\"#\">\n",
            "<span b-0yw6sxot5c=\"\" class=\"flag-icon flag-icon-bd\"></span> Bengali\r\n",
            "                            </a>, <a b-0yw6sxot5c=\"\" class=\"dropdown-item\" data-lang=\"bs\" href=\"#\">\n",
            "<span b-0yw6sxot5c=\"\" class=\"flag-icon flag-icon-ba\"></span> Bosnian\r\n",
            "                            </a>, <a b-0yw6sxot5c=\"\" class=\"dropdown-item\" data-lang=\"ca\" href=\"#\">\n",
            "<span b-0yw6sxot5c=\"\" class=\"flag-icon flag-icon-es\"></span> Catalan\r\n",
            "                            </a>]\n"
          ]
        }
      ],
      "source": [
        "# Busca y extrae todas las etiquetas <a> (enlaces) del documento HTML,\n",
        "# y luego muestra las primeras 10 para ver ejemplos de los enlaces encontrados.\n",
        "\n",
        "# Find all elements with a certain tag\n",
        "a_tags = soup.find_all(\"a\")\n",
        "print(a_tags[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yz0KNO8LgECI"
      },
      "source": [
        "Because `find_all()` is the most popular method in the Beautiful Soup search API, you can use a shortcut for it. If you treat the BeautifulSoup object as though it were a function, then it‚Äôs the same as calling `find_all()` on that object.\n",
        "\n",
        "These two lines of code are equivalent:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "LopKswYEgECI",
        "outputId": "be1b3a60-2b5e-4fff-a22c-092b4934fd66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<a b-0yw6sxot5c=\"\" class=\"dropdown-item\" data-lang=\"en\" href=\"#\">\n",
            "<span b-0yw6sxot5c=\"\" class=\"flag-icon flag-icon-us\"></span> English\r\n",
            "                            </a>\n",
            "<a b-0yw6sxot5c=\"\" class=\"dropdown-item\" data-lang=\"en\" href=\"#\">\n",
            "<span b-0yw6sxot5c=\"\" class=\"flag-icon flag-icon-us\"></span> English\r\n",
            "                            </a>\n"
          ]
        }
      ],
      "source": [
        "# Extrae todas las etiquetas <a> del HTML usando dos m√©todos equivalentes:\n",
        "# find_all(\"a\") y pasando directamente \"a\" al objeto soup.\n",
        "# Luego imprime el primer enlace encontrado con ambos m√©todos para mostrar que son iguales.\n",
        "\n",
        "a_tags = soup.find_all(\"a\")\n",
        "a_tags_alt = soup(\"a\")\n",
        "print(a_tags[0])\n",
        "print(a_tags_alt[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoCynwhtgECI"
      },
      "source": [
        "How many links did we obtain?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68xx1TD5gECI",
        "outputId": "da19909d-6d79-4b94-9dad-ca25a1397c45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "270\n"
          ]
        }
      ],
      "source": [
        "# Imprime la cantidad total de etiquetas <a> (enlaces) encontradas en el documento HTML.\n",
        "print(len(a_tags))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saI0DG8-gECJ"
      },
      "source": [
        "That's a lot! Many elements on a page will have the same HTML tag. For instance, if you search for everything with the `a` tag, you're likely to get more hits, many of which you might not want. Remember, the `a` tag defines a hyperlink, so you'll usually find many on any given page.\n",
        "\n",
        "What if we wanted to search for HTML tags with certain attributes, such as particular CSS classes?\n",
        "\n",
        "We can do this by adding an additional argument to the `find_all`. In the example below, we are finding all the `a` tags, and then filtering those with `class_=\"sidemenu\"`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "YjUfAtYcgECJ",
        "outputId": "e40cb837-cd9c-457f-efd2-f1ac092b12c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Busca y extrae las etiquetas <a> que tienen la clase CSS 'sidemenu',\n",
        "# y luego muestra los primeros 5 resultados para revisar los enlaces del men√∫ lateral.\n",
        "\n",
        "# Get only the 'a' tags in 'sidemenu' class\n",
        "side_menus = soup(\"a\", class_=\"sidemenu\")\n",
        "side_menus[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7UPvxMSgECJ"
      },
      "source": [
        "A more efficient way to search for elements on a website is via a **CSS selector**. For this we have to use a different method called `select()`. Just pass a string into the `.select()` to get all elements with that string as a valid CSS selector.\n",
        "\n",
        "In the example above, we can use `\"a.sidemenu\"` as a CSS selector, which returns all `a` tags with class `sidemenu`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "B0e5V4x6gECJ",
        "outputId": "211cd725-68ee-49cc-b088-630101eb3504",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# Usa un selector CSS para obtener todas las etiquetas <a> con la clase 'sidemenu',\n",
        "# y muestra los primeros 5 elementos seleccionados.\n",
        "\n",
        "# Get elements with \"a.sidemenu\" CSS Selector.\n",
        "selected = soup.select(\"a.sidemenu\")\n",
        "selected[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpK68NwhgECJ"
      },
      "source": [
        "## ü•ä Challenge: Find All\n",
        "\n",
        "Use BeautifulSoup to find all the `a` elements with class `mainmenu`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnhjVZ-CgECJ"
      },
      "outputs": [],
      "source": [
        "# Aqu√≠ debes agregar tu c√≥digo para continuar con el procesamiento o an√°lisis del contenido HTML,\n",
        "# como extraer informaci√≥n espec√≠fica, filtrar datos, o realizar alguna acci√≥n con los elementos seleccionados.\n",
        "\n",
        "# YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9VPVCI3gECJ"
      },
      "source": [
        "## Step 4: Get Attributes and Text of Elements\n",
        "\n",
        "Once we identify elements, we want the access information in that element. Usually, this means two things:\n",
        "\n",
        "1. Text\n",
        "2. Attributes\n",
        "\n",
        "Getting the text inside an element is easy. All we have to do is use the `text` member of a `tag` object:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "o5nxuKnRgECJ",
        "outputId": "c7195098-e9ce-49bc-a7c0-c74c0356d48c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No se encontraron enlaces con la clase 'sidemenu'\n"
          ]
        }
      ],
      "source": [
        "# Selecciona todos los enlaces (<a>) con la clase 'sidemenu'.\n",
        "# Si se encuentran, imprime el primer enlace y su tipo de dato;\n",
        "# si no, muestra un mensaje indicando que no se encontraron enlaces con esa clase.\n",
        "\n",
        "# Get all sidemenu links as a list\n",
        "side_menu_links = soup.select(\"a.sidemenu\")\n",
        "\n",
        "if side_menu_links:\n",
        "    first_link = side_menu_links[0]\n",
        "    print(first_link)\n",
        "    print('Class: ', type(first_link))\n",
        "else:\n",
        "    print(\"No se encontraron enlaces con la clase 'sidemenu'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sVu7wUrgECJ"
      },
      "source": [
        "It's a Beautiful Soup tag! This means it has a `text` member:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "Kyc9C5yDgECJ"
      },
      "outputs": [],
      "source": [
        "# Imprime el texto visible del primer enlace encontrado con la clase 'sidemenu'.\n",
        "print(first_link.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Pa_2Dj2gECK"
      },
      "source": [
        "Sometimes we want the value of certain attributes. This is particularly relevant for `a` tags, or links, where the `href` attribute tells us where the link goes.\n",
        "\n",
        "üí° **Tip**: You can access a tag‚Äôs attributes by treating the tag like a dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "ktSKjId8gECK"
      },
      "outputs": [],
      "source": [
        "# Imprime el texto visible del primer enlace encontrado con la clase 'sidemenu'.\n",
        "print(first_link.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxE6T2smgECK"
      },
      "source": [
        "## ü•ä Challenge: Extract specific attributes\n",
        "\n",
        "Extract all `href` attributes for each `mainmenu` URL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWGzf2hcgECK"
      },
      "outputs": [],
      "source": [
        "# Aqu√≠ debes agregar tu c√≥digo para continuar con el procesamiento o an√°lisis del contenido HTML,\n",
        "# como extraer informaci√≥n espec√≠fica, filtrar datos, o realizar alguna acci√≥n con los elementos seleccionados.\n",
        "\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yd1KmyA7gECK"
      },
      "source": [
        "<a id='scrape'></a>\n",
        "\n",
        "# Scraping the Illinois General Assembly\n",
        "\n",
        "Believe it or not, those are really the fundamental tools you need to scrape a website. Once you spend more time familiarizing yourself with HTML and CSS, then it's simply a matter of understanding the structure of a particular website and intelligently applying the tools of Beautiful Soup and Python.\n",
        "\n",
        "Let's apply these skills to scrape the [Illinois 98th General Assembly](http://www.ilga.gov/senate/default.asp?GA=98).\n",
        "\n",
        "Specifically, our goal is to scrape information on each senator, including their name, district, and party."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-AGNfYIgECK"
      },
      "source": [
        "## Scrape and Soup the Webpage\n",
        "\n",
        "Let's scrape and parse the webpage, using the tools we learned in the previous section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "fFFz_7SfgECK"
      },
      "outputs": [],
      "source": [
        "# Realiza una solicitud GET a una p√°gina espec√≠fica del sitio del Senado de Illinois con un par√°metro en la URL (?GA=98).\n",
        "# Luego obtiene el contenido HTML de la respuesta y lo convierte en un objeto BeautifulSoup usando el parser 'lxml',\n",
        "# para poder analizar y extraer informaci√≥n estructurada del HTML.\n",
        "\n",
        "# Make a GET request\n",
        "req = requests.get('http://www.ilga.gov/senate/default.asp?GA=98')\n",
        "# Read the content of the server‚Äôs response\n",
        "src = req.text\n",
        "# Soup it\n",
        "soup = BeautifulSoup(src, \"lxml\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5msGc8EFgECL"
      },
      "source": [
        "## Search for the Table Elements\n",
        "\n",
        "Our goal is to obtain the elements in the table on the webpage. Remember: rows are identified by the `tr` tag. Let's use `find_all` to obtain these elements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7rZV0mlgECL"
      },
      "outputs": [],
      "source": [
        "# Busca y obtiene todas las filas de tabla (<tr>) en el documento HTML.\n",
        "# Luego muestra cu√°ntas filas se encontraron, lo cual es √∫til para saber cu√°ntos registros o entradas contiene la tabla.\n",
        "\n",
        "# Get all table row elements\n",
        "rows = soup.find_all(\"tr\")\n",
        "len(rows)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLLBKy_hgECL"
      },
      "source": [
        "‚ö†Ô∏è **Warning**: Keep in mind: `find_all` gets *all* the elements with the `tr` tag. We only want some of them. If we use the 'Inspect' function in Google Chrome and look carefully, then we can use some CSS selectors to get just the rows we're interested in. Specifically, we want the inner rows of the table:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUqdDugzgECL"
      },
      "outputs": [],
      "source": [
        "# Utiliza el selector CSS 'tr tr tr' para encontrar todas las filas de tabla que est√°n anidadas dentro de otras dos filas <tr>.\n",
        "# Esto puede servir para encontrar contenido profundamente anidado en tablas HTML complejas.\n",
        "# Luego imprime las primeras 5 filas encontradas para revisar su estructura.\n",
        "\n",
        "# Returns every ‚Äòtr tr tr‚Äô css selector in the page\n",
        "rows = soup.select('tr tr tr')\n",
        "\n",
        "for row in rows[:5]:\n",
        "    print(row, '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZZ5ZmgfgECL"
      },
      "source": [
        "It looks like we want everything after the first two rows. Let's work with a single row to start, and build our loop from there."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufpLNCmxgECL"
      },
      "outputs": [],
      "source": [
        "# Selecciona una fila espec√≠fica de la tabla (en este caso, la tercera fila)\n",
        "# y la imprime de forma estructurada y legible usando 'prettify()',\n",
        "# para examinar su contenido y estructura HTML.\n",
        "example_row = rows[2]\n",
        "print(example_row.prettify())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SefMaIrIgECL"
      },
      "source": [
        "Let's break this row down into its component cells/columns using the `select` method with CSS selectors. Looking closely at the HTML, there are a couple of ways we could do this.\n",
        "\n",
        "* We could identify the cells by their tag `td`.\n",
        "* We could use the the class name `.detail`.\n",
        "* We could combine both and use the selector `td.detail`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmYk02YlgECM"
      },
      "outputs": [],
      "source": [
        "# Recorre y muestra todas las celdas <td> dentro de la fila seleccionada, permitiendo observar su contenido individual.\n",
        "for cell in example_row.select('td'):\n",
        "    print(cell)\n",
        "print()\n",
        "\n",
        "# Recorre y muestra solo los elementos que tienen la clase 'detail', sin importar su etiqueta HTML.\n",
        "# Esto permite filtrar por clase espec√≠fica dentro de la fila.\n",
        "for cell in example_row.select('.detail'):\n",
        "    print(cell)\n",
        "print()\n",
        "\n",
        "# Recorre y muestra √∫nicamente las celdas <td> que tambi√©n tienen la clase 'detail',\n",
        "# combinando filtro por etiqueta y clase para mayor precisi√≥n.\n",
        "for cell in example_row.select('td.detail'):\n",
        "    print(cell)\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fuH1w4ggECM"
      },
      "source": [
        "We can confirm that these are all the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "TEa9sxIzgECM"
      },
      "outputs": [],
      "source": [
        "# Verifica que los tres m√©todos de selecci√≥n (todas las celdas <td>, todos los elementos con clase 'detail',\n",
        "# y todas las <td> con clase 'detail') devuelvan exactamente los mismos elementos.\n",
        "# Si alguno difiere, se lanzar√° un error. Esto confirma que, en este caso, todos los <td> tienen la clase 'detail'.\n",
        "assert example_row.select('td') == example_row.select('.detail') == example_row.select('td.detail')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtJq318jgECM"
      },
      "source": [
        "Let's use the selector `td.detail` to be as specific as possible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tu5tkZ-LgECM"
      },
      "outputs": [],
      "source": [
        "# Extrae solo las celdas <td> que tienen la clase 'detail' dentro de la fila seleccionada.\n",
        "# Esto permite enfocarse √∫nicamente en la informaci√≥n relevante contenida en esas celdas.\n",
        "\n",
        "# Select only those 'td' tags with class 'detail'\n",
        "detail_cells = example_row.select('td.detail')\n",
        "detail_cells"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwJhvzklgECM"
      },
      "source": [
        "Most of the time, we're interested in the actual **text** of a website, not its tags. Recall that to get the text of an HTML element, we use the `text` member:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OexBxWtIgECM"
      },
      "outputs": [],
      "source": [
        "# Extrae solo el texto contenido dentro de cada celda de la lista 'detail_cells',\n",
        "# creando una lista con los valores textuales de las celdas, sin las etiquetas HTML.\n",
        "# Luego imprime la lista 'row_data' con el texto limpio de las celdas seleccionadas.\n",
        "\n",
        "# Keep only the text in each of those cells\n",
        "row_data = [cell.text for cell in detail_cells]\n",
        "\n",
        "print(row_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2bR0qBRgECM"
      },
      "source": [
        "Looks good! Now we just use our basic Python knowledge to get the elements of this list that we want. Remember, we want the senator's name, their district, and their party."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Fm_wBF8gECN"
      },
      "outputs": [],
      "source": [
        "# Imprime valores espec√≠ficos de la lista 'row_data' que corresponden a ciertos datos del senador:\n",
        "# el nombre (posici√≥n 0), el distrito (posici√≥n 3) y el partido pol√≠tico (posici√≥n 4).\n",
        "print(row_data[0]) # Name\n",
        "print(row_data[3]) # District\n",
        "print(row_data[4]) # Party"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlErugVcgECN"
      },
      "source": [
        "## Getting Rid of Junk Rows\n",
        "\n",
        "We saw at the beginning that not all of the rows we got actually correspond to a senator. We'll need to do some cleaning before we can proceed forward. Take a look at some examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFjgl72SgECN"
      },
      "outputs": [],
      "source": [
        "# Imprime el contenido sin procesar (HTML) de algunas filas espec√≠ficas:\n",
        "# la primera (√≠ndice 0), la segunda (√≠ndice 1) y la √∫ltima (√≠ndice -1),\n",
        "# para observar c√≥mo var√≠a la estructura entre las diferentes filas de la tabla.\n",
        "print('Row 0:\\n', rows[0], '\\n')\n",
        "print('Row 1:\\n', rows[1], '\\n')\n",
        "print('Last Row:\\n', rows[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rs9T4JfwgECN"
      },
      "source": [
        "When we write our for loop, we only want it to apply to the relevant rows. So we'll need to filter out the irrelevant rows. The way to do this is to compare some of these to the rows we do want, see how they differ, and then formulate that in a conditional.\n",
        "\n",
        "As you can imagine, there a lot of possible ways to do this, and it'll depend on the website. We'll show some here to give you an idea of how to do this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afCrRsrFgECN"
      },
      "outputs": [],
      "source": [
        "# Mide y muestra la cantidad de elementos (generalmente celdas <td> o <th>) dentro de ciertas filas para comparar su estructura.\n",
        "# Las primeras dos filas (posiblemente encabezados o filas irregulares) tienen menos elementos (filas \"malas\").\n",
        "# Las filas posteriores tienen m√°s elementos, indicando que contienen datos completos (filas \"buenas\").\n",
        "\n",
        "# Bad rows\n",
        "print(len(rows[0]))\n",
        "print(len(rows[1]))\n",
        "\n",
        "# Good rows\n",
        "print(len(rows[2]))\n",
        "print(len(rows[3]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1I-0em5jgECN"
      },
      "source": [
        "Perhaps good rows have a length of 5. Let's check:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncgwwUQWgECN"
      },
      "outputs": [],
      "source": [
        "# Filtra las filas que tienen exactamente 5 elementos (celdas), asumiendo que estas son las filas \"buenas\" con datos completos.\n",
        "# Luego imprime la primera, la pen√∫ltima y la √∫ltima fila filtrada para verificar el contenido.\n",
        "good_rows = [row for row in rows if len(row) == 5]\n",
        "\n",
        "# Let's check some rows\n",
        "print(good_rows[0], '\\n')\n",
        "print(good_rows[-2], '\\n')\n",
        "print(good_rows[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cG3hSapWgECO"
      },
      "source": [
        "We found a footer row in our list that we'd like to avoid. Let's try something else:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pKh5xZWgECO"
      },
      "outputs": [],
      "source": [
        "# Selecciona todas las celdas <td> con la clase 'detail' dentro de la tercera fila de la tabla,\n",
        "# para enfocarse en las celdas que contienen informaci√≥n relevante o detallada.\n",
        "rows[2].select('td.detail')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53MJtJQbgECO"
      },
      "outputs": [],
      "source": [
        "# Muestra las celdas con clase 'detail' de una fila \"mala\" (la √∫ltima), que posiblemente no contiene datos √∫tiles,\n",
        "# y luego las celdas de una fila \"buena\" (la sexta), que s√≠ tienen informaci√≥n relevante.\n",
        "# Despu√©s, filtra todas las filas que contienen al menos una celda con clase 'detail',\n",
        "# qued√°ndose solo con las filas que contienen datos √∫tiles.\n",
        "# Finalmente, imprime la primera y √∫ltima fila filtrada para revisi√≥n.\n",
        "\n",
        "# Bad row\n",
        "print(rows[-1].select('td.detail'), '\\n')\n",
        "\n",
        "# Good row\n",
        "print(rows[5].select('td.detail'), '\\n')\n",
        "\n",
        "# How about this?\n",
        "good_rows = [row for row in rows if row.select('td.detail')]\n",
        "\n",
        "print(\"Checking rows...\\n\")\n",
        "print(good_rows[0], '\\n')\n",
        "print(good_rows[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGva7IsQgECO"
      },
      "source": [
        "Looks like we found something that worked!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6RnyS3sgECO"
      },
      "source": [
        "## Loop it All Together\n",
        "\n",
        "Now that we've seen how to get the data we want from one row, as well as filter out the rows we don't want, let's put it all together into a loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "VBc8meKRgECO"
      },
      "outputs": [],
      "source": [
        "# Inicializa una lista vac√≠a para almacenar la informaci√≥n de los senadores.\n",
        "# Filtra las filas v√°lidas que contienen celdas con clase 'detail' para evitar filas irrelevantes o vac√≠as.\n",
        "# Recorre cada fila v√°lida y extrae el texto de las celdas con clase 'detail'.\n",
        "# Luego asigna el nombre, distrito (convertido a entero) y partido pol√≠tico a variables espec√≠ficas.\n",
        "# Finalmente, crea una tupla con esa informaci√≥n y la agrega a la lista 'members'.\n",
        "\n",
        "# Define storage list\n",
        "members = []\n",
        "\n",
        "# Get rid of junk rows\n",
        "valid_rows = [row for row in rows if row.select('td.detail')]\n",
        "\n",
        "# Loop through all rows\n",
        "for row in valid_rows:\n",
        "    # Select only those 'td' tags with class 'detail'\n",
        "    detail_cells = row.select('td.detail')\n",
        "    # Keep only the text in each of those cells\n",
        "    row_data = [cell.text for cell in detail_cells]\n",
        "    # Collect information\n",
        "    name = row_data[0]\n",
        "    district = int(row_data[3])\n",
        "    party = row_data[4]\n",
        "    # Store in a tuple\n",
        "    senator = (name, district, party)\n",
        "    # Append to list\n",
        "    members.append(senator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uoOiEr0VgECO"
      },
      "outputs": [],
      "source": [
        "# Muestra la cantidad total de senadores almacenados en la lista 'members'.\n",
        "# Se espera que el resultado sea 61, que ser√≠a el n√∫mero correcto de miembros.\n",
        "\n",
        "# Should be 61\n",
        "len(members)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXpTn6bcgECO"
      },
      "source": [
        "Let's take a look at what we have in `members`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3E0_kNbgECP"
      },
      "outputs": [],
      "source": [
        "# Imprime las primeras 5 tuplas de la lista 'members' para revisar los datos de los primeros senadores extra√≠dos.\n",
        "print(members[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T582K8SsgECP"
      },
      "source": [
        "## ü•ä  Challenge: Get `href` elements pointing to members' bills\n",
        "\n",
        "The code above retrieves information on:  \n",
        "\n",
        "- the senator's name,\n",
        "- their district number,\n",
        "- and their party.\n",
        "\n",
        "We now want to retrieve the URL for each senator's list of bills. Each URL will follow a specific format.\n",
        "\n",
        "The format for the list of bills for a given senator is:\n",
        "\n",
        "`http://www.ilga.gov/senate/SenatorBills.asp?GA=98&MemberID=[MEMBER_ID]&Primary=True`\n",
        "\n",
        "to get something like:\n",
        "\n",
        "`http://www.ilga.gov/senate/SenatorBills.asp?MemberID=1911&GA=98&Primary=True`\n",
        "\n",
        "in which `MEMBER_ID=1911`.\n",
        "\n",
        "You should be able to see that, unfortunately, `MEMBER_ID` is not currently something pulled out in our scraping code.\n",
        "\n",
        "Your initial task is to modify the code above so that we also **retrieve the full URL which points to the corresponding page of primary-sponsored bills**, for each member, and return it along with their name, district, and party.\n",
        "\n",
        "Tips:\n",
        "\n",
        "* To do this, you will want to get the appropriate anchor element (`<a>`) in each legislator's row of the table. You can again use the `.select()` method on the `row` object in the loop to do this ‚Äî similar to the command that finds all of the `td.detail` cells in the row. Remember that we only want the link to the legislator's bills, not the committees or the legislator's profile page.\n",
        "* The anchor elements' HTML will look like `<a href=\"/senate/Senator.asp/...\">Bills</a>`. The string in the `href` attribute contains the **relative** link we are after. You can access an attribute of a BeatifulSoup `Tag` object the same way you access a Python dictionary: `anchor['attributeName']`. See the <a href=\"http://www.crummy.com/software/BeautifulSoup/bs4/doc/#tag\">documentation</a> for more details.\n",
        "* There are a _lot_ of different ways to use BeautifulSoup to get things done. whatever you need to do to pull the `href` out is fine.\n",
        "\n",
        "The code has been partially filled out for you. Fill it in where it says `#YOUR CODE HERE`. Save the path into an object called `full_path`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "yNmSFMGkgECP"
      },
      "outputs": [],
      "source": [
        "# Realiza una solicitud GET a la p√°gina web espec√≠fica y obtiene el contenido HTML.\n",
        "# Usa BeautifulSoup para parsear el HTML.\n",
        "# Selecciona las filas anidadas con el selector CSS 'tr tr tr'.\n",
        "# Filtra filas que contienen celdas con clase 'detail' para eliminar filas irrelevantes.\n",
        "# Luego, recorre cada fila v√°lida para extraer texto de las celdas relevantes,\n",
        "# asigna nombre, distrito y partido, y prepara una variable 'full_path' (vac√≠a por ahora).\n",
        "# Finalmente, guarda toda la informaci√≥n en una tupla y la a√±ade a la lista 'members'.\n",
        "\n",
        "# Make a GET request\n",
        "req = requests.get('http://www.ilga.gov/senate/default.asp?GA=98')\n",
        "# Read the content of the server‚Äôs response\n",
        "src = req.text\n",
        "# Soup it\n",
        "soup = BeautifulSoup(src, \"lxml\")\n",
        "# Create empty list to store our data\n",
        "members = []\n",
        "\n",
        "# Returns every ‚Äòtr tr tr‚Äô css selector in the page\n",
        "rows = soup.select('tr tr tr')\n",
        "# Get rid of junk rows\n",
        "rows = [row for row in rows if row.select('td.detail')]\n",
        "\n",
        "# Loop through all rows\n",
        "for row in rows:\n",
        "    # Select only those 'td' tags with class 'detail'\n",
        "    detail_cells = row.select('td.detail')\n",
        "    # Keep only the text in each of those cells\n",
        "    row_data = [cell.text for cell in detail_cells]\n",
        "    # Collect information\n",
        "    name = row_data[0]\n",
        "    district = int(row_data[3])\n",
        "    party = row_data[4]\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    full_path = ''\n",
        "\n",
        "    # Store in a tuple\n",
        "    senator = (name, district, party, full_path)\n",
        "    # Append to list\n",
        "    members.append(senator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "GGrDM1H6gECP"
      },
      "outputs": [],
      "source": [
        "# Descomenta esta l√≠nea para mostrar las primeras 5 tuplas en la lista 'members' y verificar los datos extra√≠dos.\n",
        "# Uncomment to test\n",
        "# members[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvqNXtcxgECP"
      },
      "source": [
        "## ü•ä  Challenge: Modularize Your Code\n",
        "\n",
        "Turn the code above into a function that accepts a URL, scrapes the URL for its senators, and returns a list of tuples containing information about each senator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "Qd4k6g43gECQ"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "  # Define una funci√≥n que recibe una URL, realiza el scraping de los miembros del senado\n",
        "# y devuelve una lista con la informaci√≥n de cada senador (nombre, distrito, partido, full_path).\n",
        "def get_members(url):\n",
        "    # Aqu√≠ deber√≠as completar el c√≥digo para:\n",
        "    # - hacer la solicitud GET a la URL\n",
        "    # - parsear el contenido con BeautifulSoup\n",
        "    # - extraer y filtrar las filas relevantes\n",
        "    # - obtener los datos necesarios y guardarlos en una lista\n",
        "    # Finalmente, devolver la lista con los miembros.\n",
        "    return [___]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "-Z3zCnOGgECQ"
      },
      "outputs": [],
      "source": [
        "# Prueba la funci√≥n 'get_members' con la URL especificada,\n",
        "# almacena la lista de senadores en 'senate_members' y muestra la cantidad total obtenida.\n",
        "\n",
        "# Test your code\n",
        "url = 'http://www.ilga.gov/senate/default.asp?GA=98'\n",
        "senate_members = get_members(url)\n",
        "len(senate_members)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JH39VNmqgECQ"
      },
      "source": [
        "## ü•ä Take-home Challenge: Writing a Scraper Function\n",
        "\n",
        "We want to scrape the webpages corresponding to bills sponsored by each bills.\n",
        "\n",
        "Write a function called `get_bills(url)` to parse a given bills URL. This will involve:\n",
        "\n",
        "  - requesting the URL using the <a href=\"http://docs.python-requests.org/en/latest/\">`requests`</a> library\n",
        "  - using the features of the `BeautifulSoup` library to find all of the `<td>` elements with the class `billlist`\n",
        "  - return a _list_ of tuples, each with:\n",
        "      - description (2nd column)\n",
        "      - chamber (S or H) (3rd column)\n",
        "      - the last action (4th column)\n",
        "      - the last action date (5th column)\n",
        "      \n",
        "This function has been partially completed. Fill in the rest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "b1WCkjbfgECQ"
      },
      "outputs": [],
      "source": [
        "# Define una funci√≥n que recibe una URL, descarga y analiza el contenido HTML,\n",
        "# busca todas las filas de una tabla y extrae informaci√≥n relevante de cada fila\n",
        "# sobre proyectos de ley (bill_id, descripci√≥n, c√°mara, √∫ltima acci√≥n y fecha de √∫ltima acci√≥n).\n",
        "# Finalmente, devuelve una lista con tuplas que contienen estos datos.\n",
        "\n",
        "def get_bills(url):\n",
        "    src = requests.get(url).text\n",
        "    soup = BeautifulSoup(src, \"lxml\")  # Es mejor especificar el parser\n",
        "    rows = soup.select('tr')\n",
        "    bills = []\n",
        "    for row in rows:\n",
        "        # YOUR CODE HERE\n",
        "        # Aqu√≠ debes extraer los datos espec√≠ficos de cada fila:\n",
        "        # por ejemplo, usando row.select('td') para obtener las celdas,\n",
        "        # y luego acceder a su texto o atributos.\n",
        "        bill_id = None  # Reemplaza con la extracci√≥n real\n",
        "        description = None\n",
        "        chamber = None\n",
        "        last_action = None\n",
        "        last_action_date = None\n",
        "\n",
        "        bill = (bill_id, description, chamber, last_action, last_action_date)\n",
        "        bills.append(bill)\n",
        "    return bills\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "aXLxLAm1gECQ"
      },
      "outputs": [],
      "source": [
        "# Descomenta estas l√≠neas para probar la funci√≥n 'get_bills' usando la URL almacenada en la posici√≥n 3\n",
        "# del primer miembro de 'senate_members' y mostrar las primeras 5 entradas de proyectos de ley extra√≠das.\n",
        "\n",
        "# Uncomment to test your code\n",
        "# test_url = senate_members[0][3]\n",
        "# get_bills(test_url)[0:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Viy8ceRkgECQ"
      },
      "source": [
        "### Scrape All Bills\n",
        "\n",
        "Finally, create a dictionary `bills_dict` which maps a district number (the key) onto a list of bills (the value) coming from that district. You can do this by looping over all of the senate members in `members_dict` and calling `get_bills()` for each of their associated bill URLs.\n",
        "\n",
        "**NOTE:** please call the function `time.sleep(1)` for each iteration of the loop, so that we don't destroy the state's web site."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "oaHczd8FgECQ"
      },
      "outputs": [],
      "source": [
        "# Aqu√≠ debes escribir el c√≥digo que falta para realizar la tarea que necesitas,\n",
        "# como crear una estructura, procesar datos o cualquier otra operaci√≥n que est√©s implementando.\n",
        "# Por ejemplo, podr√≠as crear un diccionario para almacenar los proyectos de ley con sus √≠ndices:\n",
        "# bills_dict = {index: bill for index, bill in enumerate(bills_list)}\n",
        "# Reemplaza esta l√≠nea con el c√≥digo que desees agregar.\n",
        "\n",
        "# YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "eSjO0DMSgECQ"
      },
      "outputs": [],
      "source": [
        "# Descomenta esta l√≠nea para acceder y mostrar la informaci√≥n del proyecto de ley con √≠ndice 52 en el diccionario 'bills_dict'.\n",
        "# bills_dict[52]"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "b6f9fe9f4b7182690503d8ecc2bae97b0ee3ebf54e877167ae4d28c119a56988"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}